{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 4471234,
     "sourceType": "datasetVersion",
     "datasetId": 1031
    }
   ],
   "dockerImageVersionId": 31012,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T04:31:12.961087Z",
     "iopub.execute_input": "2025-05-07T04:31:12.961736Z",
     "iopub.status.idle": "2025-05-07T04:31:12.965824Z",
     "shell.execute_reply.started": "2025-05-07T04:31:12.961713Z",
     "shell.execute_reply": "2025-05-07T04:31:12.965075Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-09T01:50:30.224031900Z",
     "start_time": "2025-05-09T01:50:26.886236Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "SEQUENCE_LENGTH = 5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T05:03:00.399653Z",
     "iopub.execute_input": "2025-05-07T05:03:00.399918Z",
     "iopub.status.idle": "2025-05-07T05:03:00.403980Z",
     "shell.execute_reply.started": "2025-05-07T05:03:00.399902Z",
     "shell.execute_reply": "2025-05-07T05:03:00.403377Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-09T01:51:15.498987200Z",
     "start_time": "2025-05-09T01:51:15.477130400Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"retailrocket/ecommerce-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T05:03:02.299451Z",
     "iopub.execute_input": "2025-05-07T05:03:02.299716Z",
     "iopub.status.idle": "2025-05-07T05:03:02.461085Z",
     "shell.execute_reply.started": "2025-05-07T05:03:02.299699Z",
     "shell.execute_reply": "2025-05-07T05:03:02.460358Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-08T03:08:10.543121100Z",
     "start_time": "2025-05-08T03:08:10.492983600Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkagglehub\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Download latest version\u001B[39;00m\n\u001B[0;32m      4\u001B[0m path \u001B[38;5;241m=\u001B[39m kagglehub\u001B[38;5;241m.\u001B[39mdataset_download(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretailrocket/ecommerce-dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'kagglehub'"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "df = pd.read_csv('events.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "print(df.columns)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T05:03:04.386577Z",
     "iopub.execute_input": "2025-05-07T05:03:04.386828Z",
     "iopub.status.idle": "2025-05-07T05:03:05.144224Z",
     "shell.execute_reply.started": "2025-05-07T05:03:04.386813Z",
     "shell.execute_reply": "2025-05-07T05:03:05.143621Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-09T01:51:21.218479100Z",
     "start_time": "2025-05-09T01:51:20.407220700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'visitorid', 'event', 'itemid', 'transactionid'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "view_df = df[df['event'] == 'view'].sample(frac=0.08, random_state=42)\n",
    "non_view_df = df[df['event'] != 'view']\n",
    "df = pd.concat([view_df, non_view_df])\n",
    "print(f\"downsampling {len(df)}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T05:03:06.792016Z",
     "iopub.execute_input": "2025-05-07T05:03:06.792663Z",
     "iopub.status.idle": "2025-05-07T05:03:07.283528Z",
     "shell.execute_reply.started": "2025-05-07T05:03:06.792641Z",
     "shell.execute_reply": "2025-05-07T05:03:07.282833Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-09T01:51:24.607073700Z",
     "start_time": "2025-05-09T01:51:24.231102700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampling 304934\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "def create_features(df):\n",
    "    df.sort_values(['visitorid', 'timestamp'], inplace=True)\n",
    "    df['time_diff'] = df.groupby('visitorid')['timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "    df['action_count'] = df.groupby('visitorid').cumcount() + 1\n",
    "    df['cart_abandon'] = ((df['event'] == 'addtocart') & (df['event'].shift(-1) != 'transaction')).astype(int).fillna(0)\n",
    "    df['time_since_cart'] = df.groupby('visitorid')['timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "    df['long_cart_abandon'] = ((df['event'] == 'addtocart') & (df['time_since_cart'] > 300)).astype(int)\n",
    "\n",
    "    for col in ['event', 'itemid']:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "df = create_features(df)\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T05:03:09.516585Z",
     "iopub.execute_input": "2025-05-07T05:03:09.516895Z",
     "iopub.status.idle": "2025-05-07T05:03:10.279060Z",
     "shell.execute_reply.started": "2025-05-07T05:03:09.516873Z",
     "shell.execute_reply": "2025-05-07T05:03:10.278504Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-09T01:51:41.714110100Z",
     "start_time": "2025-05-09T01:51:41.135461900Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "FEATURE_COLUMNS = ['time_diff', 'action_count', 'cart_abandon', 'long_cart_abandon', 'itemid']\n",
    "TARGET_COLUMN = 'event'"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T05:03:37.963080Z",
     "iopub.execute_input": "2025-05-07T05:03:37.963370Z",
     "iopub.status.idle": "2025-05-07T05:03:37.966837Z",
     "shell.execute_reply.started": "2025-05-07T05:03:37.963352Z",
     "shell.execute_reply": "2025-05-07T05:03:37.966271Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-09T01:51:56.145594900Z",
     "start_time": "2025-05-09T01:51:56.125947400Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def build_sequences(df, max_users=None):\n",
    "    X, y = [], []\n",
    "    grouped = df.groupby('visitorid')\n",
    "    if max_users:\n",
    "        grouped = list(grouped)[:max_users]\n",
    "\n",
    "    for uid, user_df in tqdm(grouped, desc=\"Building sequences\"):\n",
    "        user_df = user_df.sort_values('timestamp')\n",
    "        features = user_df[FEATURE_COLUMNS].values\n",
    "        targets = user_df[TARGET_COLUMN].values\n",
    "        for i in range(len(features) - SEQUENCE_LENGTH):\n",
    "            X.append(features[i:i + SEQUENCE_LENGTH])\n",
    "            y.append(targets[i + SEQUENCE_LENGTH])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = build_sequences(df)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T05:03:39.473112Z",
     "iopub.execute_input": "2025-05-07T05:03:39.474061Z",
     "iopub.status.idle": "2025-05-07T05:06:10.029701Z",
     "shell.execute_reply.started": "2025-05-07T05:03:39.474034Z",
     "shell.execute_reply": "2025-05-07T05:06:10.028958Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-09T01:53:57.690361Z",
     "start_time": "2025-05-09T01:52:00.176455400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sequences: 100%|██████████| 205155/205155 [01:56<00:00, 1767.52it/s]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "class EventDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(EventDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(EventDataset(X_test, y_test), batch_size=BATCH_SIZE)\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = torch.tensor(weights, dtype=torch.float32).to(DEVICE)\n",
    "print(\"✅ 类别权重:\", class_weights)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T05:08:28.883078Z",
     "iopub.execute_input": "2025-05-07T05:08:28.883426Z",
     "iopub.status.idle": "2025-05-07T05:08:28.897461Z",
     "shell.execute_reply.started": "2025-05-07T05:08:28.883378Z",
     "shell.execute_reply": "2025-05-07T05:08:28.896646Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-09T01:55:46.131303300Z",
     "start_time": "2025-05-09T01:55:46.079791900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 类别权重: tensor([0.7818, 1.3461, 1.0224])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        x = self.relu(self.fc1(h_n[-1]))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = LSTMClassifier(input_dim=X.shape[2], hidden_dim=64, output_dim=len(classes)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T05:08:30.757439Z",
     "iopub.execute_input": "2025-05-07T05:08:30.758186Z",
     "iopub.status.idle": "2025-05-07T05:08:30.767548Z",
     "shell.execute_reply.started": "2025-05-07T05:08:30.758169Z",
     "shell.execute_reply": "2025-05-07T05:08:30.766851Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-09T01:55:58.401772500Z",
     "start_time": "2025-05-09T01:55:47.448045600Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"📈 Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 11. 验证效果\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "print(\" Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(\" F1 Score:\", f1_score(all_labels, all_preds, average='macro'))\n",
    "print(\" Classification Report:\\n\", classification_report(all_labels, all_preds))\n",
    "\n",
    "# 12. 导出为 ONNX\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, SEQUENCE_LENGTH, X.shape[2]).to(DEVICE)\n",
    "torch.onnx.export(\n",
    "    model, dummy_input, \"user_behavior_predictor.onnx\",\n",
    "    input_names=[\"input\"], output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    opset_version=11\n",
    ")\n",
    "\n",
    "print(\" model saved user_behavior_predictor.onnx\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-07T05:08:32.680621Z",
     "iopub.execute_input": "2025-05-07T05:08:32.680890Z",
     "iopub.status.idle": "2025-05-07T05:09:23.934888Z",
     "shell.execute_reply.started": "2025-05-07T05:08:32.680873Z",
     "shell.execute_reply": "2025-05-07T05:09:23.933991Z"
    },
    "ExecuteTime": {
     "end_time": "2025-05-09T01:57:21.273359600Z",
     "start_time": "2025-05-09T01:57:16.544136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Epoch 1, Loss: 259.5338\n",
      "📈 Epoch 2, Loss: 260.1338\n",
      "📈 Epoch 3, Loss: 259.6490\n",
      "📈 Epoch 4, Loss: 258.9954\n",
      "📈 Epoch 5, Loss: 259.1168\n",
      " Accuracy: 0.4703432609793034\n",
      " F1 Score: 0.3943581898233086\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.70      0.56      3354\n",
      "           1       0.28      0.09      0.13      1944\n",
      "           2       0.52      0.46      0.49      2626\n",
      "\n",
      "    accuracy                           0.47      7924\n",
      "   macro avg       0.42      0.42      0.39      7924\n",
      "weighted avg       0.44      0.47      0.43      7924\n",
      "\n",
      " model saved user_behavior_predictor.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\96450\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4661: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import onnx\n",
    "model = onnx.load(\"user_behavior_predictor.onnx\")\n",
    "onnx.checker.check_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T03:13:31.763185600Z",
     "start_time": "2025-05-09T03:13:31.738977500Z"
    }
   }
  }
 ]
}
